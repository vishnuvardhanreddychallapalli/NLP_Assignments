{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Bag-of-Words Classifiers\n",
    "\n",
    "Lecture 2 | CMU ANLP Spring 2025 | Instructor: Sean Welleck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for [CMU CS11-711 Advanced NLP](https://cmu-l3.github.io/anlp-spring2025/) that trains neural network classifiers. Specifically, each model uses a bag-of-words variant to encode an input sequence into a continuous vector that is mapped to a probability distribution over the output classes. The model is trained to minimize cross-entropy loss using backpropagation.\n",
    "\n",
    "*Ackowledgements*: adapted from Graham Neubig's ANLP Fall 2025 [code](https://github.com/neubig/anlp-code/tree/main/02-textclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet classification\n",
    "\n",
    "We use the [`mteb/tweet_sentiment_extraction`](https://huggingface.co/datasets/mteb/tweet_sentiment_extraction) dataset, which consists of classifying an input tweet as positive, neutral, or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"cb774db0d1\",\"text\":\" I`d have responded, if I were going\",\"label\":1,\"label_text\":\"neutral\"}\n",
      "{\"id\":\"549e992a42\",\"text\":\" Sooo SAD I will miss you here in San Diego!!!\",\"label\":0,\"label_text\":\"negative\"}\n",
      "{\"id\":\"088c60f138\",\"text\":\"my boss is bullying me...\",\"label\":0,\"label_text\":\"negative\"}\n",
      "{\"id\":\"9642c003ef\",\"text\":\" what interview! leave me alone\",\"label\":0,\"label_text\":\"negative\"}\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"4f4c4fc327\",\"text\":\" I`ve wondered about rake to.  The client has made it clear .NET only, don`t force devs to learn a new lang  #agile #ccnet\",\"label\":0,\"label_text\":\"negative\"}\n",
      "{\"id\":\"f67aae2310\",\"text\":\" Yay good for both of you. Enjoy the break - you probably need it after such hectic weekend  Take care hun xxxx\",\"label\":2,\"label_text\":\"positive\"}\n",
      "{\"id\":\"ed167662a5\",\"text\":\" But it was worth it  ****.\",\"label\":2,\"label_text\":\"positive\"}\n",
      "{\"id\":\"6f7127d9d7\",\"text\":\"   All this flirting going on - The ATG smiles. Yay.  ((hugs))\",\"label\":1,\"label_text\":\"neutral\"}"
     ]
    }
   ],
   "source": [
    "!tail -n 4 train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a tokenizer\n",
    "\n",
    "Based on the examples above, splitting on whitespace isn't a great idea. Let's learn a BPE vocabulary using `sentencepiece`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: bow_tokenizer_txt.txt\n",
      "  input_format: text\n",
      "  model_prefix: bow_tok\n",
      "  model_type: BPE\n",
      "  vocab_size: 2048\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: bow_tokenizer_txt.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 27480 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding met"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import json\n",
    "\n",
    "with open(\"bow_tokenizer_txt.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    with open('train.jsonl', \"r\") as f2:\n",
    "        for line in f2:\n",
    "            j = json.loads(line)\n",
    "            words = j['text']\n",
    "            f.write(words + \"\\n\")\n",
    "\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  input=\"bow_tokenizer_txt.txt\",\n",
    "  input_format=\"text\",\n",
    "  model_prefix=\"bow_tok\", \n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=2048,\n",
    "  byte_fallback=True,\n",
    "  num_threads=os.cpu_count()\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['get', 1000],\n",
       " ['‚ñÅgl', 1001],\n",
       " ['‚ñÅaway', 1002],\n",
       " ['eeee', 1003],\n",
       " ['‚ñÅleft', 1004],\n",
       " ['‚ñÅmothers', 1005],\n",
       " ['?!', 1006],\n",
       " ['ily', 1007],\n",
       " ['oke', 1008],\n",
       " ['url', 1009],\n",
       " ['‚ñÅlate', 1010],\n",
       " ['ire', 1011],\n",
       " ['hes', 1012],\n",
       " ['ner', 1013],\n",
       " ['‚ñÅHope', 1014],\n",
       " ['‚ñÅTwitter', 1015],\n",
       " ['‚ñÅsha', 1016],\n",
       " ['‚ñÅbu', 1017],\n",
       " ['‚ñÅem', 1018],\n",
       " ['inking', 1019]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bow_tok.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab[1000:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[615, 1540, 325, 505, 305, 1452, 816, 498, 1969, 1975, 1988, 578, 406, 505, 305, 1452, 399, 269, 282, 663, 358, 1968]\n",
      "‚ñÅMy\n",
      "‚ñÅname\n",
      "‚ñÅis\n",
      "‚ñÅR\n",
      "ed\n",
      "dy\n",
      "‚ñÅV\n",
      "ish\n",
      "n\n",
      "u\n",
      "v\n",
      "ard\n",
      "han\n",
      "‚ñÅR\n",
      "ed\n",
      "dy\n",
      "‚ñÅC\n",
      "ha\n",
      "ll\n",
      "ap\n",
      "all\n",
      "i\n",
      "[615, 1540, 325, 505, 305, 1452, 816, 498, 1969, 1975, 1988, 578, 406, 505, 305, 1452, 399, 269, 282, 663, 358, 1968]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1881534\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9547% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=85\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999547\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 27480 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 27480\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 52237\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=39146 min_freq=25\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11046 size=20 all=3616 active=1838 piece=or\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6531 size=40 all=4824 active=3046 piece=ow\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4212 size=60 all=5873 active=4095 piece=ic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3100 size=80 all=6938 active=5160 piece=id\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2508 size=100 all=7946 active=6168 piece=all\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2453 min_freq=155\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2002 size=120 all=9115 active=2083 piece=‚ñÅbut\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1713 size=140 all=9892 active=2860 piece=ks\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1400 size=160 all=10723 active=3691 piece=‚ñÅare\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1224 size=180 all=11428 active=4396 piece=ent\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1106 size=200 all=12047 active=5015 piece=‚ñÅsee\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1095 min_freq=142\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=971 size=220 all=12720 active=1663 piece=‚ñÅas\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=882 size=240 all=13425 active=2368 piece=ish\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=821 size=260 all=14092 active=3035 piece=‚ñÅdid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=753 size=280 all=14855 active=3798 piece=‚ñÅnight\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=707 size=300 all=15637 active=4580 piece=ving\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=704 min_freq=121\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=642 size=320 all=16149 active=1489 piece=ard\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=577 size=340 all=16626 active=1966 piece=ies\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=527 size=360 all=17106 active=2446 piece=mm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=499 size=380 all=17584 active=2924 piece=ays\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=469 size=400 all=17915 active=3255 piece=‚ñÅtonight\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=466 min_freq=108\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=438 size=420 all=18491 active=1575 piece=ile\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=420 size=440 all=19013 active=2097 piece=‚ñÅthough\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=397 size=460 all=19481 active=2565 piece=‚ñÅway\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=374 size=480 all=20025 active=3109 piece=ople\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=353 size=500 all=20301 active=3385 piece=‚ñÅqu\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=352 min_freq=94\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=333"
     ]
    }
   ],
   "source": [
    "#print(dir(sp))\n",
    "print(sp.tokenize(\"My name is Reddy Vishnuvardhan Reddy Challapalli\"))\n",
    "#print(sp.get_piece_size())\n",
    "#print(len(vocab))\n",
    "words=[pp[0] for pp in vocab]\n",
    "#print(words)\n",
    "#print(\"Challapalli\" in words,\"   \",sp.get_piece_size(\"Challapalli\"))\n",
    "mytokens=sp.tokenize(\"My name is Reddy Vishnuvardhan Reddy Challapalli\")\n",
    "for tk in mytokens:\n",
    "    print(sp.id_to_piece(tk))\n",
    "print(sp.encode(\"My name is Reddy Vishnuvardhan Reddy Challapalli\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading\n",
    "\n",
    "Read in the data, tokenize it, and split it into a training and dev set. There is a separate test set on [HuggingFace](https://huggingface.co/datasets/mteb/tweet_sentiment_extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " size=520 all=20617 active=1309 piece=ree\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=316 size=540 all=21039 active=1731 piece=ts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=299 size=560 all=21589 active=2281 piece=age\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=287 size=580 all=21877 active=2569 piece=‚ñÅhate\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=272 size=600 all=22316 active=3008 piece=rd\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=272 min_freq=78\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=264 size=620 all=22803 active=1580 piece=‚ñÅ?\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=250 size=640 all=23175 active=1952 piece=‚ñÅWe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=239 size=660 all=23423 active=2200 piece=ooo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=229 size=680 all=23839 active=2616 piece=‚ñÅWh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=222 size=700 all=24054 active=2831 piece=self\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=222 min_freq=65\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=213 size=720 all=24183 active=1321 piece=ta\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=206 size=740 all=24514 active=1652 piece=‚ñÅdis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=198 size=760 all=24854 active=1992 piece=‚ñÅem\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=191 size=780 all=25060 active=2198 piece=‚ñÅ;\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=183 size=800 all=25238 active=2376 piece=ix\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=183 min_freq=58\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=178 size=820 all=25575 active=1558 piece=ince\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=173 size=840 all=25822 active=1805 piece=ody\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=167 size=860 all=26099 active=2082 piece=‚ñÅthr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=163 size=880 all=26284 active=2267 piece=EA\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=158 size=900 all=26689 active=2672 piece=‚ñÅyesterday\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=157 min_freq=51\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=920 all=26772 active=1417 piece=‚ñÅmean\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=146 size=940 all=27033 active=1678 piece=ets\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=143 size=960 all=27349 active=1994 piece=oc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=138 size=980 all=27773 active=2418 piece=anc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=1000 all=27979 active=2624 piece=‚ñÅmight\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=136 min_freq=45\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=1020 all=28094 active=1513 piece=tern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=1040 all=28264 active=1683 piece=‚ñÅeve\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=125 size=1060 all=28534 active=1953 piece=less\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=1080 all=28858 active=2277 piece=‚ñÅTw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=120 size=1100 all=29187 active=2606 piece=way\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=120 min_freq=40\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=1120 all=29306 active=1552 piece=‚ñÅsing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=114 size=1140 all=29719 active=1965 piece=oot\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=1160 all=29879 active=2125 piece=‚ñÅmissing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=109 size=1180 all=30041 active=2287 piece=‚ñÅprobably\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=1200 all=30310 active=2556 piece=‚ñÅhit\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=106 min_freq=37\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=103 size=1220 all=30408 active=1609 piece=ty\n",
      "bpe_model_trainer.cc(268) LOG"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([332, 918, 332, 1994, 2006, 273, 512, 499, 301, 583, 312, 332, 296, 381, 394, 1883, 507], 0), ([309, 1639, 551, 325, 271, 1270, 272, 335, 321], 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(INFO) Added: freq=101 size=1240 all=30683 active=1884 piece=‚ñÅthese\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=1260 all=30835 active=2036 piece=oring\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=94 size=1280 all=31165 active=2366 piece=‚ñÅOMG\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=1300 all=31297 active=2498 piece=IT\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=92 min_freq=34\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=1320 all=31610 active=1845 piece=ower\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=1340 all=31799 active=2034 piece=‚ñÅAw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=86 size=1360 all=31982 active=2217 piece=‚ñÅshopping\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=1380 all=32233 active=2468 piece=‚ñÅPl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=83 size=1400 all=32395 active=2630 piece=‚ñÅhurt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=83 min_freq=31\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=1420 all=32540 active=1762 piece=‚ñÅonline\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=1440 all=32750 active=1972 piece=‚ñÅhi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=79 size=1460 all=32979 active=2201 piece=‚ñÅDe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=1480 all=33222 active=2444 piece=‚ñÅclose\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=76 size=1500 all=33323 active=2545 piece=‚ñÅmil\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=76 min_freq=29\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=1520 all=33463 active=1796 piece=ici\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=1540 all=33682 active=2015 piece=‚ñÅcur\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=1560 all=33857 active=2190 piece=ible\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70 size=1580 all=33973 active=2306 piece=‚ñÅcouple\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=1600 all=34181 active=2514 piece=‚ñÅfinish\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=69 min_freq=27\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=1620 all=34373 active=1901 piece=‚ñÅlisten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=1640 all=34503 active=2031 piece=‚ñÅperfect\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=1660 all=34631 active=2159 piece=hone\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=1680 all=34788 active=2316 piece=atter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=1700 all=34959 active=2487 piece=‚ñÅforget\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=63 min_freq=25\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: bow_tok.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: bow_tok.vocab\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "label_to_text = {}\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            words = j['text']\n",
    "            label = j['label']\n",
    "            label_to_text[label] = j['label_text']\n",
    "            tokens = sp.encode(words)\n",
    "            yield (tokens, label)\n",
    "\n",
    "# Read in the data\n",
    "ds = list(read_dataset(\"train.jsonl\"))\n",
    "print(ds[1:3])\n",
    "random.shuffle(ds)\n",
    "#train = ds[:-1000]\n",
    "#dev = ds[1000:]\n",
    "\n",
    "train=ds[:1000]\n",
    "test=ds[1000:]\n",
    "\n",
    "nwords = len(sp)\n",
    "ntags = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Bag-of-Embeddings\n",
    "\n",
    "Our simplest model simply sums together 3-dimensional word embeddings (3 dimensions since we have three classes).\n",
    "\n",
    "First, for understanding purposes let's implement our own embedding layer.\n",
    "\n",
    "To do so, we multiply a one-hot vector representation of a token with a suitably-sized weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[402, 510, 953, 428, 413]\n",
      "38    16\n",
      "2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 2048])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(train[1][0][:5])\n",
    "print(len(train[1][0]),\"  \",len(train[0][0]))\n",
    "print(nwords)\n",
    "\n",
    "torch.nn.functional.one_hot(torch.tensor(train[0][0]), num_classes=nwords).shape\n",
    "torch.nn.functional.one_hot(torch.tensor(train[1][0]), num_classes=nwords).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "weight = nn.Parameter(torch.randn(nwords, 64))\n",
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = torch.nn.functional.one_hot(torch.tensor(train[0][0]), num_classes=nwords)\n",
    "print(xs.shape)\n",
    "\n",
    "torch.matmul(xs.float(), weight).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(vocab_size, emb_size))\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = torch.nn.functional.one_hot(x, num_classes=self.vocab_size).float()\n",
    "        return torch.matmul(xs, self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is our simple bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels):\n",
    "        super(BoW, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, num_labels)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        #print(\"TOKENS.........\",tokens)\n",
    "        emb = self.embedding(tokens)\n",
    "        #print(\"Embedding shape.....\",emb)\n",
    "        out = torch.sum(emb, dim=0) \n",
    "        #print(\"Output shape.........\",out)\n",
    "        logits = out.view(1, -1) \n",
    "        #print(\"Logits shape.........\",logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also implement cross-entropy loss ourselves this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(logits, target):\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "    loss = -log_probs[:, target]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a training loop.\n",
    "\n",
    "We simply do \"single batch\" training here, i.e. loop over each training example one at a time and perform an update. We'll implement batching later on.\n",
    "\n",
    "You can use the SGD (Stochastic Gradient Descent) optimizer that was introduced in class, or this typically better optimizer Adam (we'll see it in a later class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=1.0835, time=0.63s\n",
      "iter 0: valid acc=0.4544\n",
      "iter 1: train loss/sent=0.9576, time=0.64s\n",
      "iter 1: valid acc=0.4880\n",
      "iter 2: train loss/sent=0.8560, time=0.62s\n",
      "iter 2: valid acc=0.5007\n",
      "iter 3: train loss/sent=0.7742, time=0.62s\n",
      "iter 3: valid acc=0.5077\n",
      "iter 4: train loss/sent=0.7051, time=0.61s\n",
      "iter 4: valid acc=0.5082\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# initialize the model\n",
    "model = BoW(nwords, ntags)\n",
    "criterion = ce_loss\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for ITER in range(5):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for x, y in train:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor([y])\n",
    "        #print(\"IN TRAIN.......\")\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
    "                ITER, train_loss/len(train), time.time()-start))\n",
    "    # Perform validation\n",
    "    test_correct = 0.0\n",
    "    for x, y in test:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        #print(\"IN TEST.......\")\n",
    "        #print(model(x)[0].detach())\n",
    "        logits = model(x)[0].detach()\n",
    "        predict = logits.argmax().item()\n",
    "        if predict == y:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: valid acc=%.4f\" % (ITER, test_correct/len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Bag-of-embeddings + output layer\n",
    "\n",
    "This is what we called `CBoW` in the lecture. Take a look at the code to see how it differs from the previous model.\n",
    "\n",
    "Also, it turns out to be important to initialize the weights well. We'll discuss this in a later class. Try removing the `nn.init` lines and see the performance change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, emb_size):\n",
    "        super(CBoW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.output_layer = nn.Linear(emb_size, num_labels)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        #print(\"The tokens..........\",tokens)\n",
    "        emb = self.embedding(tokens)    # [len(tokens) x emb_size]\n",
    "        #print(\"The emb output is..........\",emb)\n",
    "        emb_sum = torch.sum(emb, dim=0) # [emb_size]\n",
    "        h = emb_sum.view(1, -1)         # [1 x emb_size]\n",
    "        logits = self.output_layer(h)   # [1 x num_labels]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=1.0625, time=0.72s\n",
      "iter 0: dev acc=0.4932\n",
      "iter 1: train loss/sent=0.6967, time=0.91s\n",
      "iter 1: dev acc=0.5147\n",
      "iter 2: train loss/sent=0.4483, time=0.91s\n",
      "iter 2: dev acc=0.5066\n",
      "iter 3: train loss/sent=0.2670, time=0.91s\n",
      "iter 3: dev acc=0.5064\n",
      "iter 4: train loss/sent=0.1632, time=0.90s\n",
      "iter 4: dev acc=0.5055\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE=32\n",
    "model = CBoW(nwords, ntags, EMB_SIZE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for ITER in range(5):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for x, y in train:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor([y])\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
    "                ITER, train_loss/len(train), time.time()-start))\n",
    "    model.eval()\n",
    "    # Perform testing\n",
    "    test_correct = 0.0\n",
    "    for x, y in test:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        logits = model(x)[0].detach()\n",
    "        predict = logits.argmax().item()\n",
    "        if predict == y:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: dev acc=%.4f\" % (ITER, test_correct/len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Deep CBoW\n",
    "\n",
    "Now we introduce a nonlinear layer involving a tanh activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCBoW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, emb_size, hid_size):\n",
    "        super(DeepCBoW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.linear1 = nn.Linear(emb_size, hid_size)    \n",
    "        self.output_layer = nn.Linear(hid_size, num_labels)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)     \n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        emb = self.embedding(tokens)\n",
    "        emb_sum = torch.sum(emb, dim=0) \n",
    "        h = emb_sum.view(1, -1) \n",
    "        h = torch.tanh(self.linear1(h))  \n",
    "        logits = self.output_layer(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss/sent=1.0615, time=0.95s\n",
      "iter 0: dev acc=0.5027\n",
      "epoch 1: train loss/sent=0.6741, time=0.97s\n",
      "iter 1: dev acc=0.5205\n",
      "epoch 2: train loss/sent=0.3286, time=0.94s\n",
      "iter 2: dev acc=0.5068\n",
      "epoch 3: train loss/sent=0.1459, time=0.93s\n",
      "iter 3: dev acc=0.5109\n",
      "epoch 4: train loss/sent=0.0567, time=0.93s\n",
      "iter 4: dev acc=0.5051\n",
      "epoch 5: train loss/sent=0.0191, time=0.92s\n",
      "iter 5: dev acc=0.5066\n",
      "epoch 6: train loss/sent=0.0071, time=0.93s\n",
      "iter 6: dev acc=0.5055\n",
      "epoch 7: train loss/sent=0.0029, time=0.93s\n",
      "iter 7: dev acc=0.5078\n",
      "epoch 8: train loss/sent=0.0013, time=0.93s\n",
      "iter 8: dev acc=0.5058\n",
      "epoch 9: train loss/sent=0.0007, time=0.93s\n",
      "iter 9: dev acc=0.5067\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE=32\n",
    "model = DeepCBoW(nwords, ntags, EMB_SIZE, 32)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for EPOCH in range(10):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for x, y in train:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor([y])\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
    "                EPOCH, train_loss/len(train), time.time()-start))\n",
    "    model.eval()\n",
    "    # Perform testing\n",
    "    test_correct = 0.0\n",
    "    for x, y in test:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        logits = model(x)[0].detach()\n",
    "        predict = logits.argmax().item()\n",
    "        if predict == y:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: dev acc=%.4f\" % (EPOCH, test_correct/len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify an example with our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"I'm learning so much in advanced NLP!\"\n",
    "tokens = torch.tensor(sp.encode(tweet), dtype=torch.long)\n",
    "logits = model(tokens)[0].detach()\n",
    "predict = logits.argmax().item()\n",
    "label_to_text[predict]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested exercises\n",
    "\n",
    "- Try changing the initialization of weights. Does the loss and/or dev accuracy change?\n",
    "- Generalize the `DeepCBoW` implementation to take in a `num_layers` parameter. How does performance change as the number of layers is increased?\n",
    "- Try different hyperparameters (e.g., learning rate, embedding size, hidden size, number of epochs). Can you identify any consistent trends?\n",
    "- Try out different qualitative examples. Can you find patterns in how the model succeeds / fails?\n",
    "- Implement batching by introducing a new `[PAD]` token. Make sure to mask out vectors for pad tokens in the model forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try changing the initialization of weights. Does the loss and/or dev accuracy change?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First question try different weight initalizations....\n",
    "\n",
    "\n",
    "class DeepCBoW_WithLayers(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, emb_size, hid_size,num_layers):\n",
    "        super(DeepCBoW_WithLayers, self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.linear1 = nn.Linear(emb_size, hid_size)    \n",
    "        self.output_layer = nn.Linear(hid_size, num_labels)\n",
    "        self.linear_list=nn.ModuleList([nn.Linear(hid_size,hid_size) for _ in range(num_layers)])\n",
    "        nn.init.xavier_normal_(self.embedding.weight)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)     \n",
    "        nn.init.xavier_normal_(self.output_layer.weight)\n",
    "        for i in range(self.num_layers):\n",
    "            nn.init.xavier_normal_(self.linear_list[i].weight)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        emb = self.embedding(tokens)\n",
    "        emb_sum = torch.sum(emb, dim=0) \n",
    "        h = emb_sum.view(1, -1) \n",
    "        h=self.linear1(h)\n",
    "        for i in range(self.num_layers):\n",
    "            h=self.linear_list[i](h)\n",
    "        h = torch.tanh(h)  \n",
    "        logits = self.output_layer(h)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE=32\n",
    "model = DeepCBoW_WithLayers(nwords, ntags, EMB_SIZE, 32,10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss/sent=1.0897, time=2.25s\n",
      "iter 0: dev acc=0.4015\n",
      "epoch 1: train loss/sent=0.8040, time=2.33s\n",
      "iter 1: dev acc=0.5041\n",
      "epoch 2: train loss/sent=0.4514, time=2.28s\n",
      "iter 2: dev acc=0.4982\n",
      "epoch 3: train loss/sent=0.2620, time=2.26s\n",
      "iter 3: dev acc=0.5056\n",
      "epoch 4: train loss/sent=0.1702, time=2.26s\n",
      "iter 4: dev acc=0.5081\n",
      "epoch 5: train loss/sent=0.1521, time=2.28s\n",
      "iter 5: dev acc=0.5164\n",
      "epoch 6: train loss/sent=0.1422, time=3.08s\n",
      "iter 6: dev acc=0.5064\n",
      "epoch 7: train loss/sent=0.0996, time=3.14s\n",
      "iter 7: dev acc=0.4950\n",
      "epoch 8: train loss/sent=0.0681, time=3.16s\n",
      "iter 8: dev acc=0.4917\n",
      "epoch 9: train loss/sent=0.1146, time=3.16s\n",
      "iter 9: dev acc=0.4896\n"
     ]
    }
   ],
   "source": [
    "for EPOCH in range(10):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for x, y in train:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor([y])\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
    "                EPOCH, train_loss/len(train), time.time()-start))\n",
    "    model.eval()\n",
    "    # Perform testing\n",
    "    test_correct = 0.0\n",
    "    for x, y in test:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        logits = model(x)[0].detach()\n",
    "        predict = logits.argmax().item()\n",
    "        if predict == y:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: dev acc=%.4f\" % (EPOCH, test_correct/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"My name is Reddy Vishnuvardhan Reddy Challapalli and i am also called as Salar!!!!!\"\n",
    "tokens = torch.tensor(sp.encode(tweet), dtype=torch.long)\n",
    "logits = model(tokens)[0].detach()\n",
    "predict = logits.argmax().item()\n",
    "label_to_text[predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"My name is Reddy Vishnuvardhan Reddy Challapalli and i am also called as the One and only one Violent Man!!!!!\"\n",
    "tokens = torch.tensor(sp.encode(tweet), dtype=torch.long)\n",
    "logits = model(tokens)[0].detach()\n",
    "predict = logits.argmax().item()\n",
    "label_to_text[predict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to pad each sequence and mask out Vectors during the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First question try different weight initalizations....\n",
    "class DeepCBoW_WithLayers(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, emb_size, hid_size,num_layers,padding_idx):\n",
    "        super(DeepCBoW_WithLayers, self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.pad_idx=padding_idx\n",
    "        self.embedding = nn.Embedding(vocab_size+3, emb_size, padding_idx=self.pad_idx)\n",
    "        self.embedding.weight.data[self.pad_idx]=0 # This is padding index embedding which will not add any value....\n",
    "        self.linear1 = nn.Linear(emb_size, hid_size)    \n",
    "        self.output_layer = nn.Linear(hid_size, num_labels)\n",
    "        self.linear_list=nn.ModuleList([nn.Linear(hid_size,hid_size) for _ in range(num_layers)])\n",
    "        nn.init.xavier_normal_(self.embedding.weight)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)     \n",
    "        nn.init.xavier_normal_(self.output_layer.weight)\n",
    "        for i in range(self.num_layers):\n",
    "            nn.init.xavier_normal_(self.linear_list[i].weight)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        #print(\"IMMEDIDATE EMBEEDING..........\",self.embedding.weight.data[2049])\n",
    "        #print(\"SILLY EMBEDDING.................\",self.embedding(torch.tensor(2049)))\n",
    "        #https://discuss.pytorch.org/t/restrict-backpropagation-in-specific-indices-in-nn-embedding/21448/3(The idea of nulling out [PAD] embedding while computing the output....\n",
    "        vocab_size = self.embedding.weight.data.size(0)\n",
    "        ids = torch.arange(0, vocab_size)\n",
    "        mask = ids < self.pad_idx\n",
    "        mask = mask.unsqueeze(1)\n",
    "        weight = self.embedding.weight.data\n",
    "        weight = weight * mask\n",
    "        self.embedding.weight.data = weight\n",
    "        emb = self.embedding(torch.tensor(tokens))\n",
    "        #print(emb)\n",
    "        #print(\"NEXT IMMEDIDATE EMBEEDING..........\",self.embedding.weight.data[2049])\n",
    "        #print(\"next SILLY EMBEDDING.................\",self.embedding(torch.tensor(2049)))\n",
    "        emb_sum = torch.sum(emb, dim=1) \n",
    "        h = emb_sum.view(emb_sum.shape[0], -1) \n",
    "        h=self.linear1(h)\n",
    "        for i in range(self.num_layers):\n",
    "            h=self.linear_list[i](h)\n",
    "        h = torch.tanh(h)  \n",
    "        logits = self.output_layer(h)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrupt Train & Test Data with some padding set up a random padding indexes at different positions...\n",
    "EMB_SIZE=32\n",
    "model = DeepCBoW_WithLayers(nwords, ntags, EMB_SIZE, 32,10,2049)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1960, 312, 370, 303, 598, 996, 372, 1400, 1990, 277, 1031, 1119, 418, 1002, 910, 1117, 1990, 311, 266, 356, 799, 541, 868, 1975, 311, 771, 270, 350, 291, 307, 378, 1368, 261, 1552, 349, 1383, 1979]\n",
      "MAXLEN after train.......... 70    <class 'list'>\n",
      "MAXLEN after test......... 94\n"
     ]
    }
   ],
   "source": [
    "# Pad all the training and testing data before performing batch wise processing....\n",
    "print(train[0][0])\n",
    "#find the maximum train sequence length0 and pad them with extra zeros.........\n",
    "maxlen=0\n",
    "for x,y in train:\n",
    "    maxlen=max(maxlen,len(x))\n",
    "print(\"MAXLEN after train..........\",maxlen,\"  \",type(x))\n",
    "for x,y in test:\n",
    "    maxlen=max(maxlen,len(x))\n",
    "print(\"MAXLEN after test.........\",maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([332, 918, 332, 1994, 2006, 273, 512, 499, 301, 583, 312, 332, 296, 381, 394, 1883, 507], 0), ([309, 1639, 551, 325, 271, 1270, 272, 335, 321], 0)]\n",
      "13\n",
      "[273, 1989, 1974, 356, 339, 1185, 901, 305, 1990, 585, 273, 753, 485]\n",
      "NWORDS,,,,,,,,,,,,, 2048\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "ds = list(read_dataset(\"train.jsonl\"))\n",
    "print(ds[1:3])\n",
    "\n",
    "train=ds[:1000]\n",
    "test=ds[1000:]\n",
    "\n",
    "nwords = len(sp)\n",
    "ntags = 3\n",
    "\n",
    "print(len(ds[0][0]))\n",
    "print(ds[0][0])\n",
    "print(\"NWORDS,,,,,,,,,,,,,\",nwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before len is............. 27481    13    17\n",
      "ZEORLEN......... []    1\n",
      "13\n",
      "[273, 1989, 1974, 356, 339, 1185, 901, 305, 1990, 585, 273, 753, 485]\n",
      "MXTOKEN........... 2047\n",
      "DS SHAPE IS.......... 27481    13    17\n",
      "DS SHAPE IS.......... 27481    94    94\n",
      "MINLEN IS............. 0\n"
     ]
    }
   ],
   "source": [
    "# padding all the dataset with maxlen\n",
    "print(\"Before len is.............\",len(ds),\"  \",len(ds[0][0]),\"  \",len(ds[1][0]))\n",
    "import numpy as np\n",
    "mxtoken=0\n",
    "nds=[]\n",
    "minlen=1e9\n",
    "for i in range(len(ds)):\n",
    "    minlen=min(minlen,len(ds[i][0]))\n",
    "    if(len(ds[i][0])==0):\n",
    "        print(\"ZEORLEN.........\",ds[i][0],\"  \",ds[i][1])\n",
    "    padlen=maxlen-len(ds[i][0])\n",
    "    for val in ds[i][0]:\n",
    "        mxtoken=max(mxtoken,val)\n",
    "    nds.append((ds[i][0]+[nwords+1]*padlen,ds[i][1]))\n",
    "print(len(ds[0][0]))\n",
    "print(ds[0][0])\n",
    "print(\"MXTOKEN...........\",mxtoken)\n",
    "print(\"DS SHAPE IS..........\",len(ds),\"  \",len(ds[0][0]),\"  \",len(ds[1][0]))\n",
    "print(\"DS SHAPE IS..........\",len(ds),\"  \",len(nds[0][0]),\"  \",len(nds[1][0]))\n",
    "print(\"MINLEN IS.............\",minlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in nds:\n",
    "    if(len(x)<94):\n",
    "        print(\"Found value....................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[273, 1989, 1974, 356, 339, 1185, 901, 305, 1990, 585, 273, 753, 485]\n"
     ]
    }
   ],
   "source": [
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(2050, 32)\n"
     ]
    }
   ],
   "source": [
    "dummyembed=nn.Embedding(nwords+2, 32)\n",
    "print(dummyembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5967, -0.2450, -1.5582, -0.9639, -0.9788,  0.1469, -2.0398, -0.4725,\n",
      "          0.4339, -1.3042, -1.7656,  0.0544,  0.3211,  1.3244, -0.9877,  0.4795,\n",
      "         -0.0845, -0.7555, -0.0022,  1.9366,  0.6541,  1.0746, -0.4496,  0.8735,\n",
      "          0.9208, -1.5479,  1.2979,  1.2941,  0.4048,  0.6982, -0.7297,  0.3867]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(2050, 32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dummyembed(torch.tensor([2049])))\n",
    "dummyembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(2050, 32, padding_idx=2049)\n"
     ]
    }
   ],
   "source": [
    "dummyembed=nn.Embedding(nwords+2, 32,padding_idx=2049)\n",
    "print(dummyembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(2050, 32, padding_idx=2049)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dummyembed(torch.tensor([2049])))\n",
    "dummyembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=nds[:1000]\n",
    "test=nds[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([273, 1989, 1974, 356, 339, 1185, 901, 305, 1990, 585, 273, 753, 485, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049], 1)\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    train[i]=(torch.tensor(train[i][0]),torch.tensor(train[i][1]))\n",
    "for i in range(len(test)):\n",
    "    test[i]=(torch.tensor(test[i][0]),torch.tensor(test[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Assuming train_dataset is already defined\n",
    "batch_size = 16  # You can adjust the batch size as needed\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE=32\n",
    "model = DeepCBoW_WithLayers(nwords+2, ntags, EMB_SIZE, 32,10,2049)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_441187/4080918137.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = self.embedding(torch.tensor(tokens))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss/sent=1.1031, time=0.23s\n",
      "iter 0: dev acc=6.6793\n",
      "epoch 1: train loss/sent=0.9542, time=0.22s\n",
      "iter 1: dev acc=7.9698\n",
      "epoch 2: train loss/sent=0.6224, time=0.23s\n",
      "iter 2: dev acc=8.1812\n",
      "epoch 3: train loss/sent=0.3165, time=0.23s\n",
      "iter 3: dev acc=8.2283\n",
      "epoch 4: train loss/sent=0.1509, time=0.22s\n",
      "iter 4: dev acc=8.3647\n",
      "epoch 5: train loss/sent=0.1076, time=0.22s\n",
      "iter 5: dev acc=8.1848\n",
      "epoch 6: train loss/sent=0.0635, time=0.18s\n",
      "iter 6: dev acc=8.1359\n",
      "epoch 7: train loss/sent=0.0393, time=0.22s\n",
      "iter 7: dev acc=8.1401\n",
      "epoch 8: train loss/sent=0.0420, time=0.23s\n",
      "iter 8: dev acc=8.1220\n",
      "epoch 9: train loss/sent=0.0300, time=0.17s\n",
      "iter 9: dev acc=8.1588\n",
      "epoch 10: train loss/sent=0.0187, time=0.16s\n",
      "iter 10: dev acc=8.1884\n",
      "epoch 11: train loss/sent=0.0105, time=0.16s\n",
      "iter 11: dev acc=8.1522\n",
      "epoch 12: train loss/sent=0.0083, time=0.16s\n",
      "iter 12: dev acc=8.1504\n",
      "epoch 13: train loss/sent=0.0053, time=0.17s\n",
      "iter 13: dev acc=8.1075\n",
      "epoch 14: train loss/sent=0.0042, time=0.16s\n",
      "iter 14: dev acc=8.1087\n",
      "epoch 15: train loss/sent=0.0037, time=0.16s\n",
      "iter 15: dev acc=8.0942\n",
      "epoch 16: train loss/sent=0.0033, time=0.16s\n",
      "iter 16: dev acc=8.0797\n",
      "epoch 17: train loss/sent=0.0029, time=0.16s\n",
      "iter 17: dev acc=8.0821\n",
      "epoch 18: train loss/sent=0.0026, time=0.16s\n",
      "iter 18: dev acc=8.0779\n",
      "epoch 19: train loss/sent=0.0024, time=0.16s\n",
      "iter 19: dev acc=8.0876\n"
     ]
    }
   ],
   "source": [
    "for EPOCH in range(20):\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #model.embedding.weight.grad[model.pad_idx] = torch.zeros(EMB_SIZE)\n",
    "        #model.embedding(torch.tensor(2049))=0\n",
    "        optimizer.step()\n",
    "        #print(\"AFTER BACKPROP........\",model.embedding.weight.grad[2049],\" 8**********  \",model.embedding(torch.tensor(2049)))\n",
    "    print(\"epoch %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
    "                EPOCH, train_loss/len(train_loader), time.time()-start))\n",
    "    model.eval()\n",
    "    # Perform testing\n",
    "    test_correct = 0.0\n",
    "    for x, y in test_loader:\n",
    "        logits = model(x).detach()\n",
    "        predict = torch.argmax(logits,dim=1)\n",
    "        test_correct += torch.sum(predict==y)\n",
    "    print(\"iter %r: dev acc=%.4f\" % (EPOCH, test_correct/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
